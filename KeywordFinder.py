# imports
import sklearn
import pip

# pip.main(['install', 'sentence_transformers'])

from sklearn.feature_extraction.text import CountVectorizer
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity, pairwise_kernels, laplacian_kernel

# sample job description 1
test_text1 = """
Technologies & Tools We Use:
ML: PyTorch, DGL, NetworkX, XGboost, LightGBM, DVC, NVIDIA NeMo, HuggingFace, Weights & Biases
Deployment: Airflow, Docker, Kubernetes, AWS
Datastores: Postgres, Elasticsearch, SQLite, S3

What You’ll Do:
Conduct original research on large proprietary and open source data sets
Design, build and maintain scalable production-ready ML systems
Actively participate in the ML model lifecycle, from problem framing to training, deployment and monitoring in production
Partner with ML Operations team to deliver solutions for automating ML model lifecycle, from technical design to implementation
Work in a cross-functional team of ML engineers, Product Managers, Designers, Backend & Frontend engineers who are passionate about their product

What We Look For:
Outstanding people come from all different backgrounds, and we’re always interested in meeting talented people! Therefore, we do not require any particular credential or experience. If our work seems exciting to you, and you feel that you could excel in this position, we’d love to hear from you. That said, most successful candidates will fit the following profile, which reflects both our technical needs and team culture:

Bachelor's degree or higher with relevant classwork or internships in Machine Learning
Experience with advanced machine learning methods
Have strong statistical knowledge, intuition, and experience modeling real data
Have expertise in Python and Python-based ML frameworks (e.g., PyTorch or TensorFlow)
Demonstrated effective coding, documentation, and communication habits
Exercise strong communication skills and can effectively express even complicated methods and results to a broad, often non-technical audience
[optionally] Have published in top-tier journals and conferences in ML domain"""

# list of keywords generated by human for sample text 1
human_keywords_test1 = ['pytorch', 'dgl', 'networkx', 'xgboost', 'lightgbm', 'dvc', 'nvidia', 'nemo', 'huggingface',
                  'weights', 'biases', 'airflow', 'docker', 'kubernetes',
                  'aws', 'postgres', 'elasticsearch', 'sqlite', 's3', 'research', 'ml', 'training', 'model', 'modeling',
                  'partner', 'bachelor', 'intern', 'data',
                  'python', 'pytorch', 'tensorflow', 'statistical', 'coding', 'documentation', 'communication',
                  'methods', 'machine', 'learning', 'published', 'journals',
                  'work', 'experience', 'design']

test_text2 = """
Empower your career growth through exposure to new technologies and processes
You will work closely with the Management Analysis team to build solutions to help drive product usage and find new sales opportunities
You will collaborate with cross functional teams and vendors to enhance and ingest data from internal and external sources
You will identify opportunities to simplify and automate workflows
You will translate business requirements into robust and scalable data pipelines for key business metrics
You will use technologies such as Hadoop, Spark, Hive, Kafka and more
You will be working with structured and unstructured datasets
You will need to have:
5+ years of experience with DBMS, RDBMS and ETL methodologies.
Experience building automated, scalable architectures in an enterprise setting
Advanced SQL capabilities are required. Knowledge of database design and experience working with extremely large data volumes is a plus.
Programming experience in Python. PySpark and Scala is a plus.
Strong understanding of data warehousing methodologies, ETL processing and dimensional data modeling.
Strong problem-solving skills and trouble-shooting skills
Knowledge of Airflow and Argo is a plus
BA, BS, MS, PhD in Computer Science, Engineering or related technology field
Nice to Have:
Familiarity with Hadoop ecosystem (HDFS, Spark)
Specific expertise in implementing Informatica solutions is a plus.
Knowledge of MPP systems
Knowledge of streaming technologies like Kafka
Knowledge of business intelligence reporting tools such as QlikSense, Tableau, Power BI, Cognos
Experience working in a UNIX or Linux development environment
"""

human_keywords_test2 = ['solutions', 'build', 'sales', 'opportunities', 'teams', 'data', 'internal', 'external', 'simplify',
                        'automate', 'workflows', 'business', 'pipelines', 'metrics', 'key',
                        'hadoop', 'spark', 'hive', 'kafka', 'structured', 'unstructured', 'dataset', 'dbms', 'rdbms', 'etl',
                        'sql', 'design', 'python', 'pyspark', 'scala', 'warehousing', 'modeling', 'problem-solving', 'trouble-shooting',
                        'airflow', 'argo', 'computer', 'science', 'hdfs', 'informatica', 'mpp', 'qliksense', 'tableau',
                        'power bi', 'cognos', 'unix', 'linus']

n_gram_range = (1, 1)
stop_words = "english"

# Extract candidate words/phrases - test 1
count1 = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([test_text1])
candidates1 = count1.get_feature_names_out()

model = SentenceTransformer('distilbert-base-nli-mean-tokens')
doc_embedding = model.encode([test_text1])
candidate_embeddings = model.encode(candidates1)

top_n = 50

distances_cosine = cosine_similarity(doc_embedding, candidate_embeddings)
keywords_cosine = [candidates1[index] for index in distances_cosine.argsort()[0][-top_n:]]

metrics = {'metric': ['linear', 'polynomial', 'rbf']}

distances_kernels = pairwise_kernels(doc_embedding, candidate_embeddings, metric='rbf')
keywords_kernels = [candidates1[index] for index in distances_kernels.argsort()[0][-top_n:]]

distances_laplacian = laplacian_kernel(doc_embedding, candidate_embeddings)
keywords_laplacian = [candidates1[index] for index in distances_laplacian.argsort()[0][-top_n:]]

cosine_likeness_score = len(list(set(human_keywords_test1).intersection(keywords_cosine))) / len(human_keywords_test1)
kernels_likeness_score = len(list(set(human_keywords_test1).intersection(keywords_kernels))) / len(human_keywords_test1)
laplacian_likeness_score = len(list(set(human_keywords_test1).intersection(keywords_laplacian))) / len(human_keywords_test1)

# percent of keywords in the human list of keywords that are generated
print("Test 1 Scores:")
print("Cosine Similarity Score: ", cosine_likeness_score)
print("Pairwise Kernels (rbf) Score: ", kernels_likeness_score)
print("Laplacian Kernel Score: ", laplacian_likeness_score)

# Extract candidate words/phrases - test 2
count2 = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([test_text2])
candidates2 = count2.get_feature_names_out()

doc_embedding2 = model.encode([test_text2])
candidate_embeddings2 = model.encode(candidates2)

top_n = 50

distances_cosine2 = cosine_similarity(doc_embedding2, candidate_embeddings2)
keywords_cosine2 = [candidates2[index] for index in distances_cosine2.argsort()[0][-top_n:]]

distances_kernels2 = pairwise_kernels(doc_embedding2, candidate_embeddings2, metric='rbf')
keywords_kernels2 = [candidates2[index] for index in distances_kernels2.argsort()[0][-top_n:]]

distances_laplacian2 = laplacian_kernel(doc_embedding2, candidate_embeddings2)
keywords_laplacian2 = [candidates2[index] for index in distances_laplacian2.argsort()[0][-top_n:]]

cosine_likeness_score2 = len(list(set(human_keywords_test2).intersection(keywords_cosine2))) / len(human_keywords_test2)
kernels_likeness_score2 = len(list(set(human_keywords_test2).intersection(keywords_kernels2))) / len(human_keywords_test2)
laplacian_likeness_score2 = len(list(set(human_keywords_test2).intersection(keywords_laplacian2))) / len(human_keywords_test2)

# percent of keywords in the human list of keywords that are generated
print("")
print("Test 2 Scores:")
print("Cosine Similarity Score: ", cosine_likeness_score2)
print("Pairwise Kernels (rbf) Score: ", kernels_likeness_score2)
print("Laplacian Kernel Score: ", laplacian_likeness_score2)

# pairwise kernels (rbf) does the best, so we will use this pairwise metric to find keywords in our get_keywords function
def get_keywords(text):
    # Extract candidate words/phrases
    counts = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])
    candidates = counts.get_feature_names()

    model = SentenceTransformer('distilbert-base-nli-mean-tokens')
    doc_embedding = model.encode([text])
    candidate_embeddings = model.encode(candidates)

    top_n = 50

    distances_kernels = pairwise_kernels(doc_embedding, candidate_embeddings, metric='rbf')
    keywords_kernels = [candidates[index] for index in distances_kernels.argsort()[0][-top_n:]]

    return keywords_kernels
